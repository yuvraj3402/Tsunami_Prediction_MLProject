{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(r\"tsunami\\artifact\\data_ingestion\\2024-02-20-15-35-51\\train\\tsunami.csv\")\n",
    "features=[\"ID\",\"YEAR\",\"DAY\",\"HOUR\",\"LOCATION_NAME\",\"MINUTE\",\"LATITUDE\",\"LONGITUDE\",\"DAMAGE_TOTAL_DESCRIPTION\",\"HOUSES_TOTAL_DESCRIPTION\",\"DEATHS_TOTAL_DESCRIPTION\",\"URL\",\"COMMENTS\"]\n",
    "df=df.drop(features,axis=1)\n",
    "df=df[df[\"CAUSE\"].str.contains(\"Unknown\")==False]\n",
    "df[\"MONTH\"]=df[\"MONTH\"].map({1.0:\"January\", 2.0:\"February\",3.0: \"March\",4.0: \"April\", 5.0:\"May\", 6.0:\"June\",7.0: \"July\", 8.0:\"August\",9.0: \"September\",10.0: \"October\",11.0: \"November\",12.0: \"December\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numerical_columns=[\"EQ_MAGNITUDE\",\"EQ_DEPTH\",\"TS_INTENSITY\"]\n",
    "cat_columns=[\"MONTH\",\"COUNTRY\",\"REGION\",\"CAUSE\"]\n",
    "\n",
    "num_pipeline=Pipeline(steps=[\n",
    "                ('impute',SimpleImputer(strategy=\"median\")),\n",
    "                ('StandardScalar',StandardScaler())\n",
    "            ])\n",
    "            \n",
    "            \n",
    "\n",
    "cat_pipeline=Pipeline(steps=[\n",
    "                ('imputer',SimpleImputer(strategy=\"most_frequent\")),\n",
    "                ('OneHotEncoder',OneHotEncoder()),\n",
    "                ('scaler', StandardScaler(with_mean=False))\n",
    "            ])\n",
    "            \n",
    "\n",
    "Preprocessing=ColumnTransformer([\n",
    "                ('num_pipeline',num_pipeline,numerical_columns),\n",
    "                ('cat_pipeline',cat_pipeline,cat_columns)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.drop(\"EVENT_VALIDITY\",axis=1)\n",
    "y=df[\"EVENT_VALIDITY\"]\n",
    "x1=Preprocessing.fit_transform(X=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dill\n",
    "\n",
    "def save_object(file_path:str,obj):\n",
    "    dir_path = os.path.dirname(file_path)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    with open(file_path, \"wb\") as file_obj:\n",
    "        dill.dump(obj, file_obj)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=trans_config.preprocessed_object_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(file_path,x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsunami.components.data_ingestion import DataIngestion\n",
    "from tsunami.components.data_validation import DataValidation\n",
    "from tsunami.components.data_transformation import DataTransformation\n",
    "from tsunami.config.configuration import configuration\n",
    "config=configuration()\n",
    "\n",
    "trans_config=config.get_data_transformation_config()\n",
    "ing_obj=DataIngestion(config.get_data_ingestion_config())\n",
    "inj_art=ing_obj.initiate_data_ingestion()\n",
    "val_obj=DataValidation(config.get_data_validation_config(),\n",
    "                       inj_art)\n",
    "\n",
    "val_art=val_obj.initiate_data_validation()\n",
    "trans_obj=DataTransformation(inj_art,\n",
    "                             val_art,\n",
    "                             trans_config)\n",
    "trans_art=trans_obj.initiate_data_transformation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "def load_object(file_path:str):\n",
    "    with open(file_path, \"rb\") as file_obj:\n",
    "        return dill.load(file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def load_data(file_path):\n",
    "    df=pd.read_csv(file_path)\n",
    "    return df\n",
    "    \n",
    "\n",
    "\n",
    "def load_object(file_path:str):\n",
    "      with open(file_path, \"rb\") as file_obj:\n",
    "           return dill.load(file_obj)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=trans_art.transformed_train_file_path\n",
    "y=trans_art.target_feature_file_path\n",
    "preproce=load_object(trans_art.preprocessed_object_file_path)\n",
    "x=load_object(file_path=x)\n",
    "y=load_data(file_path=y)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model1=RandomForestClassifier(n_estimators=100, max_depth=13)\n",
    "model1=model1.fit(X=x,y=y)\n",
    "predict=model1.predict(X=x)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true=y,y_pred=predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
